{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'int'>\n",
      "0.9354838709677419\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9642857142857143\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9821428571428571\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "0.9910714285714286\n",
      "accurasy :  0.9910714285714286\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('theDataset/Eminem.csv')\n",
    "\n",
    "data = data.drop(['COMMENT_ID', 'AUTHOR', 'DATE'], axis=1)\n",
    "# CONTENT / CLASS\n",
    "\n",
    "x= data['CONTENT']\n",
    "a = 10\n",
    "print(type(a))\n",
    "y=data['CLASS']\n",
    "\n",
    "#x=x.replace(r'#([^\\s]+)', r'\\1', regex=True)\n",
    "#x=x.replace('\\'\"',regex=True)\n",
    "\n",
    "# nradj3o al data statistics \n",
    "vectorizer = CountVectorizer( )\n",
    "featuers = vectorizer.fit_transform(x) \n",
    "\n",
    "#kf = KFold(n_splits= 5, random_state=1, shuffle=True)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    " \n",
    "model = LogisticRegression()\n",
    "\n",
    " \n",
    "X_train, X_test,y_train, y_test = train_test_split(featuers,y ,\n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True)\n",
    "\n",
    "model.fit(X_train, y_train)  \n",
    "\n",
    "featuerspre = vectorizer.transform([\"dgfsgf\"]) \n",
    "\n",
    "y_pred = model.predict(featuerspre) \n",
    "\n",
    "\n",
    "import joblib\n",
    "\n",
    "iter =100\n",
    "maxacurssy = 0.9354838709677419\n",
    "bestX_train, bestX_test,besty_train,besty_test = X_train, X_test,y_train, y_test \n",
    "for i in range(iter):\n",
    "    X_train, X_test,y_train, y_test = train_test_split(featuers,y ,\n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True)\n",
    "    model.fit(X_train, y_train) \n",
    "    print(maxacurssy)\n",
    "    if maxacurssy <model.score(X_test,y_test):\n",
    "        joblib.dump(model,'nahini.joblib')\n",
    "        maxacurssy = model.score(X_test,y_test)\n",
    "        bestX_train, bestX_test,besty_train,besty_test = X_train, X_test,y_train, y_test \n",
    "\n",
    "#print(y_pred,featuerspre)\n",
    "print(\"accurasy : \",maxacurssy) \n",
    "# cfl accurasy :  0.956989247311828\n",
    "#accurasy :  0.9354838709677419\n",
    "\n",
    "#The final accurasy :  0.967741935483871"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model.joblib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-fe629458317d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimplPape\u001b[0m  \u001b[1;33m=\u001b[0m\u001b[0mjoblib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'model.joblib'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msimplPape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\asus\\anaconda3\\lib\\site-packages\\joblib\\numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(filename, mmap_mode)\u001b[0m\n\u001b[0;32m    575\u001b[0m             \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    576\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 577\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    578\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model.joblib'"
     ]
    }
   ],
   "source": [
    "simplPape  =joblib.load('model.joblib')\n",
    "print(simplPape.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"neural network\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946236559139785\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('theDataset/Shakira.csv')\n",
    "\n",
    "data = data.drop(['COMMENT_ID', 'AUTHOR', 'DATE'], axis=1)\n",
    "# CONTENT / CLASS\n",
    "\n",
    "x= data['CONTENT']\n",
    "y=data['CLASS']\n",
    "\n",
    "#x=x.replace(r'#([^\\s]+)', r'\\1', regex=True)\n",
    "#x=x.replace('\\'\"',regex=True)\n",
    "\n",
    "# nradj3o al data statistics \n",
    "vectorizer = CountVectorizer( )\n",
    "featuers = vectorizer.fit_transform(x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(model.score(X_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n",
      "test_scores : 0.07526881720430108 and train_scores : 0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "train_scores, test_scores = list(), list()\n",
    "values = [i for i in range(1, 100)]\n",
    "\n",
    "X_train, X_test,y_train, y_test = train_test_split(featuers,y ,\n",
    "                                   test_size=0.25, \n",
    "                                   shuffle=True)\n",
    "\n",
    "for i in values:\n",
    "    model = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                    hidden_layer_sizes=(5, 2), random_state=1)\n",
    "\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "\n",
    "\n",
    "    train_yhat = model.predict(X_train)\n",
    "\n",
    "    train_acc =(1/(y_train.values.size))*np.sum((train_yhat- y_train.values)**2)\n",
    "    train_scores.append(train_acc)\n",
    "\n",
    "    test_yhat = model.predict(X_test)\n",
    "    test_acc =(1/(y_test.values.size))*np.sum((test_yhat-y_test.values)**2)\n",
    "    test_scores.append(test_acc)\n",
    "    print(\"test_scores : \"+str(test_acc)+\" and train_scores : \"+str(train_acc))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(values, train_scores, '-o', label='Train')\n",
    "plt.plot(values, test_scores, '-o', label='Test')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08602150537634409\n"
     ]
    }
   ],
   "source": [
    "test_yhat = model.predict(X_test)\n",
    "test_acc =(1/(y_test.values.size))*(np.sum((test_yhat-y_test.values)**2))\n",
    "\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of data before delete the duplicated rows  (448, 5)\n",
      "the shape of data after delete the duplicated rows  (412, 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eminem dataset\n",
      "The mean accuracy is :  0.9431650485436894\n",
      "The Standard Deviation accuracy is :  0.02178889105484589\n",
      "1.0\n",
      "the prediction for 'subscribe to my channel' is :  [1]\n",
      "the prediction for 'I love this song' is :  [0]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LogisticRegression' object has no attribute 'decision_pfunction'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-f0f4301132a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m \"\"\"\n\u001b[1;32m--> 178\u001b[1;33m \u001b[0mcost_history\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_pfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;31m# Plot the cost history\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LogisticRegression' object has no attribute 'decision_pfunction'"
     ]
    }
   ],
   "source": [
    "#the Imports\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pylab as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "class Preproces: \n",
    "    def __init__(self,TheData,deletecolumnsNames =[]):\n",
    "        tp =str(type(TheData))\n",
    "        if(tp!=\"<class 'pandas.core.frame.DataFrame'>\"):\n",
    "            print('it need to be a data pandas')\n",
    "        else:\n",
    "            self.TheData = TheData\n",
    "            self.TheData = self.TheData.drop_duplicates(subset='CONTENT', keep='first')\n",
    "            self.deletecolumnsNames =deletecolumnsNames \n",
    "    \n",
    "    #drop \n",
    "    def delete(self): \n",
    " \n",
    "        for i in self.deletecolumnsNames:\n",
    "            self.TheData= self.TheData.drop([''+i], axis=1)\n",
    "\n",
    "\n",
    "    def PickY(self,columnsYName):\n",
    "\n",
    "        self.dicr={\n",
    "            'y':data[''+str(columnsYName)],\n",
    "        }\n",
    "        d = self.TheData\n",
    "        d = d.drop([''+columnsYName], axis=1)\n",
    "        i = 0\n",
    "        for col in d.columns:\n",
    "            self.dicr['x'+str(i)] = d[''+str(col)]\n",
    "            i+=1\n",
    "\n",
    " \n",
    "\n",
    "    def Lower(self):\n",
    "        for Xs in self.dicr:\n",
    "            if Xs !='y':\n",
    "                self.dicr[Xs] = self.dicr[Xs].str.lower()\n",
    "        \n",
    "    def cleaningLinks(self):\n",
    "        for Xs in self.dicr:\n",
    "            if Xs !='y':\n",
    "                self.dicr[Xs] = self.dicr[Xs].str.replace(r'.com/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),])+',' ')\n",
    "\n",
    "\n",
    "    def __remove_stop(self,x,stop_words):\n",
    "        return \" \".join([word for word in str(x).split() if word not in stop_words])\n",
    "    \n",
    "    def deletingTheStopWords(self):\n",
    "        nltk.download('stopwords')\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        for Xs in self.dicr:\n",
    "            if Xs !='y':\n",
    "                self.dicr[Xs] = self.dicr[Xs].apply(lambda x : self.__remove_stop(x,stop_words))\n",
    "    \n",
    "    def deleteUselessCaracters(self):\n",
    "        for Xs in self.dicr:\n",
    "            if Xs !='y':\n",
    "                self.dicr[Xs] = self.dicr[Xs].str.replace(r'[^\\w\\s]',' ')\n",
    "    \n",
    "    def __remove_multiple_spaces(self,x):\n",
    "        return \" \".join([word for word in str(x).split()])\n",
    "\n",
    "    def deleteDoubleSpace(self):\n",
    "        for Xs in self.dicr:\n",
    "            if Xs !='y':\n",
    "                self.dicr[Xs] = self.dicr[Xs].apply(lambda x : self.__remove_multiple_spaces(x))\n",
    "\n",
    "    def vectorization(self):\n",
    "        self.vectorizer = CountVectorizer()\n",
    "        for Xs in self.dicr:\n",
    "            if Xs !='y':\n",
    "                self.dicr[Xs] =self.vectorizer.fit_transform(self.dicr[Xs])\n",
    "\n",
    " \n",
    "    def NormalSplit(self):\n",
    "            \n",
    "        self.X_train, self.X_test,self.y_train, self.y_test = train_test_split(\n",
    "            self.dicr['x0'] ,self.dicr['y'] ,\n",
    "                            test_size=0.25, \n",
    "                            shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "    def doEverything(self,columnsYName):\n",
    "        self.delete()\n",
    "        self.PickY(columnsYName)\n",
    "        self.Lower()\n",
    "        self.cleaningLinks()\n",
    "        self.deletingTheStopWords()\n",
    "        self.deleteUselessCaracters()\n",
    "        self.deleteDoubleSpace()\n",
    "        self.vectorization()\n",
    "        self.NormalSplit()\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "data = pd.read_csv('theDataset/Eminem.csv')\n",
    "print('the shape of data before delete the duplicated rows ',data.shape )\n",
    "data = data.drop_duplicates(subset='CONTENT', keep='first') \n",
    "\n",
    "print('the shape of data after delete the duplicated rows ',data.shape )\n",
    "\n",
    "p = Preproces(data,['COMMENT_ID', 'AUTHOR', 'DATE'])\n",
    "p.doEverything('CLASS')\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "alpha = 0.03\n",
    "\n",
    "model = LogisticRegression()\n",
    " \n",
    "model.fit(p.X_train,p.y_train)\n",
    "\n",
    "joblib.dump(model,'myModel.joblib')\n",
    "\n",
    " \n",
    " \n",
    "accuracys = []\n",
    "\n",
    "iter =500\n",
    "\n",
    "for i in range(iter):\n",
    "    p.NormalSplit()\n",
    "    model.fit(p.X_train,p.y_train)\n",
    "    scor = model.score(p.X_test,p.y_test)\n",
    "    accuracys.append(scor)\n",
    "\n",
    "\n",
    "accuracys = np.array(accuracys)\n",
    " \n",
    "\n",
    "print('Eminem dataset')\n",
    "print('The mean accuracy is : ',accuracys.mean())\n",
    "\n",
    "print('The Standard Deviation accuracy is : ',accuracys.std())\n",
    "\n",
    "\n",
    "\n",
    "model = joblib.load('myModel.joblib')\n",
    " \n",
    "\n",
    "\n",
    "predict= model.predict(p.X_test)\n",
    "\n",
    "print(model.score(p.X_test,p.y_test))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "x1 = p.vectorizer.transform([\"subscribe to my channel\"])\n",
    "x2 = p.vectorizer.transform([\"I love this song\"]) \n",
    "prediction1 = model.predict(x1)\n",
    "prediction2 = model.predict(x2)\n",
    "print('the prediction for \\'subscribe to my channel\\' is : ',prediction1)\n",
    "\n",
    "print('the prediction for \\'I love this song\\' is : ',prediction2)\n",
    "\n",
    "\"\"\"\n",
    "plt.spy(p.dicr['x0'],markersize=1)\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "cost_history = model.decision_pfunction(p.X_train)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel('Number of Iterations')\n",
    "plt.ylabel('Training Error')\n",
    "plt.title('Cost History of Logistic Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, predictions)\n",
    "    print(\"Fold accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c1d5a056f04d97314a9f946bc8c5185004572d3d68312220c0ba298420421f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
